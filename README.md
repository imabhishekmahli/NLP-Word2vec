# NLP-Word2vec
# Documentatio :

In NLP, word embedding is a term used for the representation of words for text analysis,
typically in the form of a real-valued vector that encodes the meaning of the word such that the
words that are closer in the vector space are expected to be similar in the meaning.


word embedding is a technique where you convert a word into a vector

Types of word embedding --> 1. Frequency based (BoW, tfidf, glove)
				    2. Prediction based (Word2vec)

Question. What is word2vec?

Answer. It is a word embedding technique whose work is to convert the word into a collection of number.
   you can not directly apply algo to the word, first you have to convert it into number than apply the algo.

	Advantages of using Word2vec --> 1. semantic meaning {Happy, Joy}
						   2. Low dimension vectors are formed. (100 to 300)
						   3. Dense Vector (less non-zero vector, which solves the problem of overfitting)
